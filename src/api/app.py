# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MuE9HqowpbKRFFNp7u5WzAOfpzyKQox5
"""

# !pip install -U langchain langchain-openai langchain-community langgraph[all] langchain-tavily

from typing import TypedDict, Annotated, Optional
from langgraph.graph import add_messages, StateGraph, END
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from langchain_tavily import TavilySearch
from langgraph.checkpoint.memory import MemorySaver
from uuid import uuid4
import json
import asyncio

model = ChatOpenAI(
    model="gemini-2.0-flash",             # Any Gemini model
    api_key="AIzaSyD_H9ASnR8TNyfy8qj7uCVGXU-BfYzxYGs",        # The Gemini API key
    base_url="https://generativelanguage.googleapis.com/v1beta/"
)

import getpass
import os

if not os.environ.get("TAVILY_API_KEY"):
    os.environ["TAVILY_API_KEY"] = getpass.getpass("Tavily API key:\n")

search_tool = TavilySearch(
    max_results=5,
    topic="general")

tools = [search_tool]

memory = MemorySaver()

llm_with_tools = model.bind_tools(tools=tools)

from typing import TypedDict, Annotated, Optional
from langgraph.graph import add_messages, StateGraph, END
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from langchain_tavily import TavilySearch
from langgraph.checkpoint.memory import MemorySaver
from uuid import uuid4
import json
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage

class State(TypedDict):
    messages: Annotated[list, add_messages]

async def model(state: State):
    result = await llm_with_tools.ainvoke(state["messages"])
    return {
        "messages": [result],
    }

async def tools_router(state: State):
    last_message = state["messages"][-1]

    if(hasattr(last_message, "tool_calls") and len(last_message.tool_calls) > 0):
        return "tool_node"
    else:
        return END

async def tool_node(state):
    """Custom tool node that handles tool calls from the LLM."""
    tool_calls = state["messages"][-1].tool_calls

    tool_messages = [
        ToolMessage(
            content=str(await search_tool.ainvoke(tool_call["args"])),
            tool_call_id=tool_call["id"],
            name=tool_call["name"]
        )
        for tool_call in tool_calls if tool_call["name"] == "tavily_search_results_json"
    ]

    return {"messages": tool_messages}

graph_builder = StateGraph(State)

graph_builder.add_node("model", model)
graph_builder.add_node("tool_node", tool_node)
graph_builder.set_entry_point("model")

graph_builder.add_conditional_edges("model", tools_router)
graph_builder.add_edge("tool_node", "model")

graph = graph_builder.compile(checkpointer=memory)

# from IPython.display import Image, display
# from langchain_core.runnables.graph import MermaidDrawMethod

# display(
#     Image(
#         graph.get_graph().draw_mermaid_png(
#             draw_method=MermaidDrawMethod.API
#         )
#     )
# )

config = {
    "configurable": {
        "thread_id": 5
    }
}

# response = await graph.ainvoke({
#     "messages": [HumanMessage(content="When is the next spaceX launch?")],
# }, config=config)

config = {
    "configurable": {
        "thread_id": 8
    }
}

# async def tool_call_model(message):
#  for event in graph.astream_events({
#     "messages": [HumanMessage(content=f"{message}")]
# }, config=config, version="v2"):
#     if event.get("event") == "on_chat_model_stream" and "chunk" in event["data"]:
#         chunk = event["data"]["chunk"]
#         if hasattr(chunk, "content") and chunk.content:
#             yield chunk.content

async def tool_call_model(message):
    async for event in graph.astream_events({
        "messages": [HumanMessage(content=message)],
}, config=config, version="v2"):


       if (
            event.get("type") == "on_chat_model_stream" and 
            event.get("data", {}).get("chunk")
        ):
            chunk = event["data"]["chunk"]
            delta = chunk.get("content", "")
            if delta:
                yield delta


async def main():
    async for token in tool_call_model("when is the next spacex launch"):
        print(token, end="", flush=True)

asyncio.run(main())

